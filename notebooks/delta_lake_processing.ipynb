{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Data Exploration\n",
    "This notebook explores the stock market data and tests the steps before implementing them in Python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables if .env file exists\n",
    "try:\n",
    "    load_dotenv()\n",
    "except:\n",
    "    print(\"No .env file found, using default paths\")\n",
    "\n",
    "# Define data paths using environment variables or defaults\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\", \"data\")\n",
    "CSV_FILE_PATH = os.getenv(\"CSV_FILE_PATH\", os.path.join(DATA_DIR, \"tech_stocks.csv\"))\n",
    "DELTA_TABLE_PATH = os.getenv(\"DELTA_TABLE_PATH\", os.path.join(DATA_DIR, \"delta_tables/tech_stocks\"))\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(os.path.dirname(CSV_FILE_PATH), exist_ok=True)\n",
    "os.makedirs(DELTA_TABLE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 19:12:06 WARN Utils: Your hostname, abdelhalim resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface enp8s0)\n",
      "25/05/01 19:12:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/abdelhalim/Desktop/Temp%20/StockMarketAnalysis/stock/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/abdelhalim/.ivy2/cache\n",
      "The jars for the packages stored in: /home/abdelhalim/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eac916b9-324a-40ff-8a75-23f68dcc00d3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.1.0/delta-core_2.12-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (1188ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (125ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.1.0!delta-core_2.12.jar (1188ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.1.0/delta-storage-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.1.0!delta-storage.jar (125ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (210ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (180ms)\n",
      ":: resolution report :: resolve 2869ms :: artifacts dl 1724ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eac916b9-324a-40ff-8a75-23f68dcc00d3\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (3759kB/10ms)\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (210ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (180ms)\n",
      ":: resolution report :: resolve 2869ms :: artifacts dl 1724ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eac916b9-324a-40ff-8a75-23f68dcc00d3\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (3759kB/10ms)\n",
      "25/05/01 19:12:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/01 19:12:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .appName(\"DeltaLakeProcessing\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_TABLE_PATH = \"data/delta_tables\"\n",
    "CSV_FILE_PATH = \"/home/abdelhalim/Desktop/Temp /StockMarketAnalysis/data/tech_stocks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 19:12:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|Ticker|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------+\n",
      "|2024-03-01| 178.7061901764464| 179.6815803178056|176.54639012704013|  178.815673828125|73488000|  AAPL|\n",
      "|2024-03-04|175.32213749041347| 176.0686126971914|172.97322823226915|174.27708435058594|81510100|  AAPL|\n",
      "|2024-03-05|169.95748724050745| 171.2314705105848|168.82284541599856|169.32049560546875|95132400|  AAPL|\n",
      "|2024-03-06|  170.256065044738|170.43522699407845|   167.88724549652|168.32518005371094|68587700|  AAPL|\n",
      "|2024-03-07|168.35505375316498|169.92763018116116| 167.6981670423948| 168.2057647705078|71765100|  AAPL|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "print(f\"Loading data from {CSV_FILE_PATH}\")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(CSV_FILE_PATH)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add YearMonth column for partitioning\n",
    "df = df.withColumn(\"YearMonth\", col(\"Date\").substr(0, 7))\n",
    "\n",
    "# Write to Delta Lake with partitioning\n",
    "print(f\"Saving data to {DELTA_TABLE_PATH}\")\n",
    "df.write.format(\"delta\").partitionBy(\"Ticker\", \"YearMonth\").mode(\"overwrite\").save(DELTA_TABLE_PATH)\n",
    "print(\"âœ… Data partitioned and saved in Delta Lake format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data was saved correctly\n",
    "df_read = spark.read.format(\"delta\").load(DELTA_TABLE_PATH)\n",
    "print(f\"Row count: {df_read.count()}\")\n",
    "df_read.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
